{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zombie/miniconda3/envs/torch/lib/python3.8/site-packages/braindecode/datautil/preprocess.py:10: UserWarning: datautil.preprocess module is deprecated and is now under preprocessing.preprocess, please use from import braindecode.preprocessing.preprocess\n",
      "  warn('datautil.preprocess module is deprecated and is now under '\n",
      "/home/zombie/miniconda3/envs/torch/lib/python3.8/site-packages/braindecode/datautil/windowers.py:4: UserWarning: datautil.windowers module is deprecated and is now under preprocessing.windowers, please use from import braindecode.preprocessing.windowers\n",
      "  warn('datautil.windowers module is deprecated and is now under '\n"
     ]
    }
   ],
   "source": [
    "from braindecode.datasets import BaseConcatDataset, BaseDataset\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from mne.datasets.sleep_physionet.age import fetch_data\n",
    "\n",
    "\n",
    "from braindecode.datautil.preprocess import preprocess, Preprocessor\n",
    "from braindecode.datautil.windowers import create_windows_from_events\n",
    "from braindecode.datautil.preprocess import zscore\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import (\n",
    "    cohen_kappa_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mne\n",
    "from mne.datasets.sleep_physionet.age import fetch_data\n",
    "\n",
    "from braindecode.datautil.preprocess import preprocess, Preprocessor\n",
    "from braindecode.datautil.windowers import create_windows_from_events\n",
    "from braindecode.datautil.preprocess import zscore\n",
    "from braindecode.datasets import BaseConcatDataset, BaseDataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "DATA_PATH = '/mnt/scratch'\n",
    "\n",
    "\n",
    "# Params\n",
    "NEG_SAMPLES = 128 # number of negative samples per positive sample\n",
    "BATCH_SIZE = 1\n",
    "POS_MIN = 1\n",
    "NEG_MIN = 15\n",
    "EPOCH_LEN = 7\n",
    "NUM_SAMPLES = 2000\n",
    "SUBJECTS = np.arange(10)\n",
    "RECORDINGS = [1, 2]\n",
    "\n",
    "random_state = 1234\n",
    "n_jobs = 1\n",
    "sfreq = 100\n",
    "high_cut_hz = 30\n",
    "\n",
    "window_size_s = 30\n",
    "sfreq = 100\n",
    "window_size_samples = window_size_s * sfreq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4001E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4002E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4011E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4012E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4021E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4022E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4031E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4032E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4041E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4042E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4051E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4052E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4061E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4062E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4071E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4072E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4081E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4082E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4091E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /mnt/scratch/physionet-sleep-data/SC4092E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 2508000  =      0.000 ... 25080.000 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zombie/miniconda3/envs/torch/lib/python3.8/site-packages/braindecode/preprocessing/preprocess.py:52: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
      "  warn('Preprocessing choices with lambda functions cannot be saved.')\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 3348000  =      0.000 ... 33480.000 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 3261000  =      0.000 ... 32610.000 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 3447000  =      0.000 ... 34470.000 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 3060000  =      0.000 ... 30600.000 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 3027000  =      0.000 ... 30270.000 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 2850000  =      0.000 ... 28500.000 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 2706000  =      0.000 ... 27060.000 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 3705000  =      0.000 ... 37050.000 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 3606000  =      0.000 ... 36060.000 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 2010000  =      0.000 ... 20100.000 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 3741000  =      0.000 ... 37410.000 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 2481000  =      0.000 ... 24810.000 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 3036000  =      0.000 ... 30360.000 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 2907000  =      0.000 ... 29070.000 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 3813000  =      0.000 ... 38130.000 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 3381000  =      0.000 ... 33810.000 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 3159000  =      0.000 ... 31590.000 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 3423000  =      0.000 ... 34230.000 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 3345000  =      0.000 ... 33450.000 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 30 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal lowpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Upper passband edge: 30.00 Hz\n",
      "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
      "- Filter length: 45 samples (0.450 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "837 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 837 events and 3000 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "1116 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 1116 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "1088 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 1088 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "1150 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 1150 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "1021 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 1021 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "1009 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 1009 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "951 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 951 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "903 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 903 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "1235 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 1235 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "1199 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 1199 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "671 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 671 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "1246 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 1246 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "828 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 828 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "1013 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 1013 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "970 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 970 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "1272 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 1272 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "1128 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 1128 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "1054 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 1054 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "1131 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 1131 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Adding metadata with 4 columns\n",
      "Replacing existing metadata with 4 columns\n",
      "1104 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 1104 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zombie/miniconda3/envs/torch/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function zscore is deprecated; will be removed in 0.7.0. Use sklearn.preprocessing.scale instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<braindecode.datasets.base.BaseConcatDataset at 0x7f0cfb949b20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_state = 1234\n",
    "n_jobs = 1\n",
    "sfreq = 100\n",
    "high_cut_hz = 30\n",
    "\n",
    "window_size_s = 30\n",
    "sfreq = 100\n",
    "window_size_samples = window_size_s * sfreq\n",
    "\n",
    "\n",
    "\n",
    "class SleepPhysionet(BaseConcatDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        subject_ids=None,\n",
    "        recording_ids=None,\n",
    "        preload=False,\n",
    "        load_eeg_only=True,\n",
    "        crop_wake_mins=30,\n",
    "        crop=None,\n",
    "    ):\n",
    "        if subject_ids is None:\n",
    "            subject_ids = range(83)\n",
    "        if recording_ids is None:\n",
    "            recording_ids = [1, 2]\n",
    "\n",
    "        paths = fetch_data(\n",
    "            subject_ids,\n",
    "            recording=recording_ids,\n",
    "            on_missing=\"warn\",\n",
    "            path=DATA_PATH,\n",
    "        )\n",
    "\n",
    "        all_base_ds = list()\n",
    "        for p in paths:\n",
    "            raw, desc = self._load_raw(\n",
    "                p[0],\n",
    "                p[1],\n",
    "                preload=preload,\n",
    "                load_eeg_only=load_eeg_only,\n",
    "                crop_wake_mins=crop_wake_mins,\n",
    "                crop=crop,\n",
    "            )\n",
    "            base_ds = BaseDataset(raw, desc)\n",
    "            all_base_ds.append(base_ds)\n",
    "        super().__init__(all_base_ds)\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_raw(\n",
    "        raw_fname,\n",
    "        ann_fname,\n",
    "        preload,\n",
    "        load_eeg_only=True,\n",
    "        crop_wake_mins=False,\n",
    "        crop=None,\n",
    "    ):\n",
    "        ch_mapping = {\n",
    "            \"EOG horizontal\": \"eog\",\n",
    "            \"Resp oro-nasal\": \"misc\",\n",
    "            \"EMG submental\": \"misc\",\n",
    "            \"Temp rectal\": \"misc\",\n",
    "            \"Event marker\": \"misc\",\n",
    "        }\n",
    "        exclude = list(ch_mapping.keys()) if load_eeg_only else ()\n",
    "\n",
    "        raw = mne.io.read_raw_edf(raw_fname, preload=preload, exclude=exclude)\n",
    "        annots = mne.read_annotations(ann_fname)\n",
    "        raw.set_annotations(annots, emit_warning=False)\n",
    "\n",
    "        if crop_wake_mins > 0:\n",
    "            # Find first and last sleep stages\n",
    "            mask = [x[-1] in [\"1\", \"2\", \"3\", \"4\", \"R\"] for x in annots.description]\n",
    "            sleep_event_inds = np.where(mask)[0]\n",
    "\n",
    "            # Crop raw\n",
    "            tmin = annots[int(sleep_event_inds[0])][\"onset\"] - crop_wake_mins * 60\n",
    "            tmax = annots[int(sleep_event_inds[-1])][\"onset\"] + crop_wake_mins * 60\n",
    "            raw.crop(tmin=max(tmin, raw.times[0]), tmax=min(tmax, raw.times[-1]))\n",
    "\n",
    "        # Rename EEG channels\n",
    "        ch_names = {i: i.replace(\"EEG \", \"\") for i in raw.ch_names if \"EEG\" in i}\n",
    "        raw.rename_channels(ch_names)\n",
    "\n",
    "        if not load_eeg_only:\n",
    "            raw.set_channel_types(ch_mapping)\n",
    "\n",
    "        if crop is not None:\n",
    "            raw.crop(*crop)\n",
    "\n",
    "        basename = os.path.basename(raw_fname)\n",
    "        subj_nb = int(basename[3:5])\n",
    "        sess_nb = int(basename[5])\n",
    "        desc = pd.Series({\"subject\": subj_nb, \"recording\": sess_nb}, name=\"\")\n",
    "       \n",
    "\n",
    "        return raw, desc\n",
    "\n",
    "\n",
    "random_state = 1234\n",
    "n_jobs = -1\n",
    "sfreq = 100\n",
    "high_cut_hz = 30\n",
    "\n",
    "EPOCH_LEN = 15\n",
    "\n",
    "dataset = SleepPhysionet(\n",
    "    subject_ids=SUBJECTS, recording_ids=RECORDINGS, crop_wake_mins=30\n",
    ")\n",
    "\n",
    "preprocessors = [\n",
    "    Preprocessor(lambda x: x * 1e6),\n",
    "    Preprocessor(\"filter\", l_freq=None, h_freq=high_cut_hz, n_jobs=n_jobs),\n",
    "]\n",
    "\n",
    "# Transform the data\n",
    "preprocess(dataset, preprocessors)\n",
    "\n",
    "\n",
    "mapping = {  # We merge stages 3 and 4 following AASM standards.\n",
    "    \"Sleep stage W\": 0,\n",
    "    \"Sleep stage 1\": 1,\n",
    "    \"Sleep stage 2\": 2,\n",
    "    \"Sleep stage 3\": 3,\n",
    "    \"Sleep stage 4\": 3,\n",
    "    \"Sleep stage R\": 4,\n",
    "}\n",
    "\n",
    "windows_dataset = create_windows_from_events(\n",
    "    dataset,\n",
    "    trial_start_offset_samples=0,\n",
    "    trial_stop_offset_samples=0,\n",
    "    window_size_samples=window_size_samples,\n",
    "    window_stride_samples=window_size_samples,\n",
    "    preload= True,\n",
    "    mapping=mapping,\n",
    ")\n",
    "\n",
    "\n",
    "preprocess(windows_dataset, [Preprocessor(zscore)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositioningDataset(BaseConcatDataset):\n",
    "    \"\"\"BaseConcatDataset with __getitem__ that expects 2 indices and a target.\"\"\"\n",
    "\n",
    "    def __init__(self, list_of_ds, epoch_len=7):\n",
    "        super().__init__(list_of_ds)\n",
    "        self.return_pair = True\n",
    "        self.epoch_len = epoch_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        rec, pos, neg = index\n",
    "        pos_data = []\n",
    "        neg_data = []\n",
    "\n",
    "        # print(rec, pos, neg)\n",
    "\n",
    "        assert pos != neg, \"pos and neg should not be the same\"\n",
    "\n",
    "        for i in range(-(self.epoch_len // 2), self.epoch_len // 2 + 1):\n",
    "            pos_data.append(super().__getitem__(pos + i)[0])\n",
    "            neg_data.append(super().__getitem__(neg + i)[0])\n",
    "\n",
    "        pos_data = np.stack(pos_data, axis=0) # (7, 2, 3000)\n",
    "        neg_data = np.stack(neg_data, axis=0) # (7, 2, 3000)\n",
    "\n",
    "        return pos_data, neg_data\n",
    "\n",
    "\n",
    "class TuneDataset(BaseConcatDataset):\n",
    "    \"\"\"BaseConcatDataset for train and test\"\"\"\n",
    "\n",
    "    def __init__(self, list_of_ds):\n",
    "        super().__init__(list_of_ds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        X = super().__getitem__(index)[0]\n",
    "        y = super().__getitem__(index)[1]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "class RecordingSampler(Sampler):\n",
    "    def __init__(self, metadata, random_state=None, epoch_len=7):\n",
    "\n",
    "        self.metadata = metadata\n",
    "        self.epoch_len = epoch_len\n",
    "        self._init_info()\n",
    "        self.rng = check_random_state(random_state)\n",
    "\n",
    "    def _init_info(self):\n",
    "        keys = [\"subject\", \"recording\"]\n",
    "\n",
    "        self.metadata = self.metadata.reset_index().rename(\n",
    "            columns={\"index\": \"window_index\"}\n",
    "        )\n",
    "        self.info = (\n",
    "            self.metadata.reset_index()\n",
    "            .groupby(keys)[[\"index\", \"i_start_in_trial\"]]\n",
    "            .agg([\"unique\"])\n",
    "        )\n",
    "        self.info.columns = self.info.columns.get_level_values(0)\n",
    "\n",
    "    def sample_recording(self):\n",
    "        \"\"\"Return a random recording index.\"\"\"\n",
    "        return self.rng.choice(self.n_recordings)\n",
    "            \n",
    "    def __iter__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def n_recordings(self):\n",
    "        return self.info.shape[0]\n",
    "\n",
    "\n",
    "class RelativePositioningSampler(RecordingSampler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        metadata,\n",
    "        tau_pos,\n",
    "        tau_neg,\n",
    "        n_examples,\n",
    "        same_rec_neg=True,\n",
    "        random_state=None,\n",
    "        epoch_len=7,\n",
    "    ):\n",
    "        super().__init__(metadata, random_state=random_state, epoch_len=epoch_len)\n",
    "\n",
    "        self.tau_pos = tau_pos\n",
    "        self.tau_neg = tau_neg\n",
    "        self.epoch_len = epoch_len\n",
    "        self.n_examples = n_examples\n",
    "        self.same_rec_neg = same_rec_neg\n",
    "        self.info['index'] = self.info['index'].apply(lambda x: x[self.epoch_len // 2 : -(self.epoch_len // 2) ])\n",
    "        self.info['i_start_in_trial'] = self.info['i_start_in_trial'].apply(lambda x: x[self.epoch_len // 2 : -(self.epoch_len // 2) ])\n",
    "        self.info.iloc[-1]['index'] = self.info.iloc[-1]['index'][:-(7 // 2) - 1]\n",
    "        self.info.iloc[-1]['i_start_in_trial'] = self.info.iloc[-1]['i_start_in_trial'][: -(self.epoch_len // 2) - 1]\n",
    "\n",
    "    def _sample_pair(self):\n",
    "        \n",
    "        \"\"\"Sample a pair of two windows.\"\"\"\n",
    "        # Sample first window\n",
    "        \n",
    "        for rec_id in range(self.info.shape[0]):\n",
    "            epochs = self.info.iloc[rec_id][\"index\"]\n",
    "            start_trail = self.info.iloc[rec_id][\"i_start_in_trial\"]\n",
    "            for ep_id, trail in zip(epochs, start_trail):\n",
    "\n",
    "                # print(ep_id, trail)\n",
    "                win_ind1, rec_ind1 = ep_id, rec_id\n",
    "                ts1 = trail\n",
    "                ts = self.info.iloc[rec_ind1][\"i_start_in_trial\"]\n",
    "\n",
    "                epoch_min = self.info.iloc[rec_ind1][\"i_start_in_trial\"][self.epoch_len // 2]\n",
    "                epoch_max = self.info.iloc[rec_ind1][\"i_start_in_trial\"][-self.epoch_len // 2]\n",
    "\n",
    "                if self.same_rec_neg:\n",
    "                    mask = ((ts <= ts1 - self.tau_neg) & (ts >= epoch_min)) | (\n",
    "                        (ts >= ts1 + self.tau_neg) & (ts <= epoch_max)\n",
    "                    )\n",
    "                    \n",
    "                if sum(mask) == 0:\n",
    "                    raise NotImplementedError\n",
    "                win_ind2 = self.rng.choice(self.info.iloc[rec_ind1][\"index\"][mask])\n",
    "                yield rec_ind1, win_ind1, win_ind2\n",
    "\n",
    "    def __iter__(self):  \n",
    "        yield from self._sample_pair()\n",
    "      \n",
    "    def __len__(self):\n",
    "        epoch_len = 0\n",
    "        for rec_id in range(self.info.shape[0]):\n",
    "            epoch_len += len(self.info.iloc[rec_id][\"index\"])\n",
    "        return epoch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pretext_sampler\n",
    "pretext_sampler.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts1 = self.metadata.iloc[win_ind1][\"i_start_in_trial\"]\n",
    "# ts = self.info.iloc[rec_ind1][\"i_start_in_trial\"]\n",
    "\n",
    "\n",
    "a.metadata.iloc[525: 535][\"i_start_in_trial\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.info.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pretext: 6181it [00:37, 163.02it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-da86eb639f2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretext_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pretext'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mtemp_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPRETEXT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-854ae7f50954>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_len\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_len\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mpos_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mneg_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/braindecode/datasets/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "PRETEXT_PATH = os.path.join(DATA_PATH, \"pretext\")\n",
    "TEST_PATH = os.path.join(DATA_PATH, \"test\")\n",
    "\n",
    "if not os.path.exists(PRETEXT_PATH): os.mkdir(PRETEXT_PATH)\n",
    "if not os.path.exists(TEST_PATH): os.mkdir(TEST_PATH)\n",
    "\n",
    "\n",
    "splitted = dict()\n",
    "\n",
    "splitted[\"pretext\"] = RelativePositioningDataset(\n",
    "    [ds for ds in windows_dataset.datasets if ds.description[\"subject\"] in np.arange(3)],\n",
    "    epoch_len = EPOCH_LEN\n",
    ")\n",
    "\n",
    "tau_pos, tau_neg = int(sfreq * POS_MIN * 60), int(sfreq * NEG_MIN * 60)\n",
    "n_examples_pretext = NUM_SAMPLES * len(splitted[\"pretext\"].datasets)\n",
    "\n",
    "\n",
    "pretext_sampler = RelativePositioningSampler(\n",
    "    splitted[\"pretext\"].get_metadata(),\n",
    "    tau_pos=tau_pos,\n",
    "    tau_neg=tau_neg,\n",
    "    n_examples=n_examples_pretext,\n",
    "    same_rec_neg=True,\n",
    "    random_state=random_state,  # Same samples for every iteration of dataloader\n",
    "    epoch_len = 7\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Dataloader\n",
    "pretext_loader = DataLoader(\n",
    "    splitted[\"pretext\"],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=pretext_sampler\n",
    ")\n",
    "\n",
    "\n",
    "for i, arr in tqdm(enumerate(pretext_loader), desc = 'pretext'):\n",
    "    temp_path = os.path.join(PRETEXT_PATH, str(i) + '.npz')\n",
    "    np.savez(temp_path, pos = arr[0].numpy().squeeze(0), neg = arr[1].numpy().squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted[\"pretext\"][0, 55, 121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from augmentations import *\n",
    "from loss import loss_fn\n",
    "from model import sleep_model\n",
    "from train import *\n",
    "from utils import *\n",
    "\n",
    "from braindecode.util import set_random_seeds\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import wandb\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "PATH = '/mnt/scratch/sleepkfoldsame/'\n",
    "\n",
    "# Params\n",
    "SAVE_PATH = \"single-epoch-same.pth\"\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 128\n",
    "lr = 5e-4\n",
    "n_epochs = 200\n",
    "NUM_WORKERS = 5\n",
    "N_DIM = 256\n",
    "EPOCH_LEN = 7\n",
    "m = 0.9995\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "random_state = 1234\n",
    "sfreq = 100\n",
    "high_cut_hz = 30\n",
    "\n",
    "# Seeds\n",
    "rng = np.random.RandomState(random_state)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"GPU available: {torch.cuda.device_count()}\")\n",
    "\n",
    "set_random_seeds(seed=random_state, cuda=device == \"cuda\")\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "\n",
    "# Extract number of channels and time steps from dataset\n",
    "n_channels, input_size_samples = (2, 3000)\n",
    "model = sleep_model(n_channels, input_size_samples, n_dim = N_DIM)\n",
    "\n",
    "\n",
    "q_encoder = model.to(device)\n",
    "k_encoder = model.to(device)\n",
    "\n",
    "for param_q, param_k in zip(q_encoder.parameters(), k_encoder.parameters()):\n",
    "    param_k.data.copy_(param_q.data) \n",
    "    param_k.requires_grad = False  # not update by gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuneDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for train and test\"\"\"\n",
    "\n",
    "    def __init__(self, subjects):\n",
    "        self.subjects = subjects\n",
    "        self._add_subjects()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        X = self.X[index]\n",
    "        y =  self.y[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "        \n",
    "    def _add_subjects(self):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        for subject in self.subjects:\n",
    "            self.X.append(subject['windows'])\n",
    "            self.y.append(subject['y'])\n",
    "        self.X = np.concatenate(self.X, axis=0)\n",
    "        self.y = np.concatenate(self.y, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"subject\", \"recording\"]\n",
    "\n",
    "metadata1 = splitted[\"train\"].get_metadata()\n",
    "metadata1 = metadata1.reset_index().rename(\n",
    "    columns={\"index\": \"window_index\"}\n",
    ")\n",
    "info1 = (\n",
    "    metadata1.reset_index()\n",
    "    .groupby(keys)[[\"index\", \"i_start_in_trial\"]]\n",
    "    .agg([\"unique\"])\n",
    ")\n",
    "info1.columns = info1.columns.get_level_values(0)\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "rec_ind1 = rng.choice(info1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"subject\", \"recording\"]\n",
    "\n",
    "metadata = splitted[\"pretext\"].get_metadata()\n",
    "metadata = metadata.reset_index().rename(\n",
    "    columns={\"index\": \"window_index\"}\n",
    ")\n",
    "info = (\n",
    "    metadata.reset_index()\n",
    "    .groupby(keys)[[\"index\", \"i_start_in_trial\"]]\n",
    "    .agg([\"unique\"])\n",
    ")\n",
    "\n",
    "info.columns = info.columns.get_level_values(0)\n",
    "epoch_len = 7\n",
    "rng = np.random.RandomState(1234)\n",
    "rec_ind = rng.choice(info.shape[0])\n",
    "win_ind = rng.choice(\n",
    "        info.iloc[rec_ind][\"index\"][epoch_len // 2 : -epoch_len // 2]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuneDataset(BaseConcatDataset):\n",
    "    \"\"\"BaseConcatDataset for train and test\"\"\"\n",
    "\n",
    "    def __init__(self, list_of_ds):\n",
    "        super().__init__(list_of_ds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        X = super().__getitem__(index)[0]\n",
    "        y = super().__getitem__(index)[1]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "splitted[\"train\"] = TuneDataset(\n",
    "    [ds for ds in windows_dataset.datasets if ds.description[\"subject\"] in sub_train]\n",
    ")\n",
    "\n",
    "splitted[\"test\"] = TuneDataset(\n",
    "    [ds for ds in windows_dataset.datasets if ds.description[\"subject\"] in sub_test]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted[\"train\"].get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted[\"pretext\"].get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from multi_epoch.utils import CosineAnnealingWarmupRestarts\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet18()\n",
    "\n",
    "a = torch.randn((10, 10), requires_grad=True)\n",
    "b = torch.randn((10, 10), requires_grad=True)\n",
    "c = a+b\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = CosineAnnealingWarmupRestarts(optimizer, first_cycle_steps = 500, cycle_mult = 1.2, max_lr = 0.01, min_lr = 5e-6, warmup_steps = 100, gamma = 0.5)\n",
    "\n",
    "step = 0\n",
    "lrs = []\n",
    "\n",
    "for i in range(10000):\n",
    "    step += 1\n",
    "    optimizer.step()\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "    scheduler.step()\n",
    "    \n",
    "plt.plot(lrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from data_preprocessing.dataloader import data_generator,cross_data_generator,ft_data_generator\n",
    "from trainer import sleep_ft,sleep_pretrain\n",
    "from config import Config\n",
    "#path = \"/scratch/SLEEP_data/data_multi/sleepEDF/\"\n",
    "path = \"/scratch/SLEEP_data/\"\n",
    "training_mode = 'ss'\n",
    "config = Config()\n",
    "def ft_fun(file_name,epoch):\n",
    "    file_name = file_name+\"_epoch\"+str(epoch)+'.pt'\n",
    "    name = os.path.join(config.exp_path,file_name)\n",
    "    src_path = '/scratch/SLEEP_data/'\n",
    "    n = cross_data_generator(src_path,[],[],config)\n",
    "    kfold = KFold(n_splits=5,shuffle=False)\n",
    "    idxs = np.arange(0,n,1)\n",
    "    for split,(train_idx,val_idx) in enumerate(kfold.split(idxs)):\n",
    "        wandb.init(project='delete',group='K-Cross LE',job_type='split: '+str(split),notes='',name=file_name,save_code=True,entity='sleep-staging')\n",
    "        train_dl,valid_dl= cross_data_generator(src_path,train_idx,val_idx,config)\n",
    "        le_model = sleep_ft(name,config,train_dl,valid_dl,wandb)\n",
    "        lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "        le_trainer = pl.Trainer(callbacks=[lr_monitor],profiler='simple',enable_checkpointing=False,max_epochs=config.num_ft_epoch,gpus=1)\n",
    "        wandb.watch([le_model],log='all',log_freq=200)\n",
    "        le_trainer.fit(le_model)\n",
    "        wandb.finish(quiet=True)\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--file_name\", type=str, default=\"finv1_fusion_intra\",\n",
    "                    help=\"weights file name\")\n",
    "parser.add_argument(\"--epoch\", type= int, default=20,\n",
    "                    help=\"current epoch\")\n",
    "args = parser.parse_args()\n",
    "ft_fun(args.file_name,int(args.epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_data_generator(data_path,train_idxs,val_idxs,configs):\n",
    "    train_ds = torch.load(os.path.join(data_path, \"train.pt\"))\n",
    "    train_ds['samples'] = train_ds['samples'][:,0,:].unsqueeze(1)\n",
    "    valid_ds = torch.load(os.path.join(data_path, \"val.pt\"))\n",
    "    valid_ds['samples'] = valid_ds['samples'][:,0,:].unsqueeze(1)\n",
    "    train_subs = [48,72,24,30,34,50,38,15,60,12]\n",
    "    train_segs = [3937, 2161, 3448, 1783, 3083, 2429, 3647, 2714, 3392, 2029]\n",
    "    val_subs = [23,26,37,44,49,51,54,59,73,82]\n",
    "    val_segs = [2633, 2577, 2427, 2287, 2141, 2041, 2864, 3071, 4985, 3070]\n",
    "    segs = train_segs+val_segs\n",
    "    if train_idxs !=[]:\n",
    "        dataset = {}\n",
    "        train_dataset = {}\n",
    "        valid_dataset = {}\n",
    "        dataset['samples'] = torch.from_numpy(np.vstack((train_ds['samples'],valid_ds['samples'])))\n",
    "        dataset['labels'] = torch.from_numpy(np.hstack((train_ds['labels'],valid_ds['labels'])))\n",
    "        dataset['samples'] = torch.split(dataset['samples'],segs)\n",
    "        dataset['labels'] = torch.split(dataset['labels'],segs)\n",
    "        print('Split Shape',len(dataset['samples']))\n",
    "        train_dataset['samples'] = [dataset['samples'][i] for i in train_idxs]\n",
    "        train_dataset['labels'] = [dataset['labels'][i] for i in train_idxs]\n",
    "        dataset['samples'] = torch.cat(dataset['samples'])\n",
    "        dataset['labels'] = torch.cat(dataset['labels'])\n",
    "        print('Stack Shape',dataset['samples'].shape,dataset['labels'].shape)\n",
    "        print('Train Shape',train_dataset['samples'].shape,train_dataset['labels'].shape)\n",
    "        train_dataset['samples'] = torch.cat(train_dataset['samples'])\n",
    "        train_dataset['labels'] = torch.cat(train_dataset['labels'])\n",
    "        print('Train Shape',train_dataset['samples'].shape,train_dataset['labels'].shape)\n",
    "        train_dataset = Load_Dataset(train_dataset, configs,data_path=data_path,training_mode='ft')\n",
    "        valid_dataset['samples'] = [dataset['samples'][i] for i in val_idxs]\n",
    "        valid_dataset['labels'] = [dataset['labels'][i] for i in val_idxs]\n",
    "        valid_dataset['samples'] = torch.cat(valid_dataset['samples'])\n",
    "        valid_dataset['labels'] = torch.cat(valid_dataset['labels'])\n",
    "        valid_dataset = Load_Dataset(valid_dataset,configs,data_path=data_path,training_mode='ft')\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=configs.batch_size,\n",
    "                                                   shuffle=True, drop_last=configs.drop_last,\n",
    "                                                   num_workers=10,pin_memory=True,persistent_workers=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=configs.batch_size,\n",
    "                                                   shuffle=False, drop_last=configs.drop_last,\n",
    "                                                   num_workers=10,pin_memory=True,persistent_workers=True)\n",
    "        del dataset\n",
    "        del train_dataset\n",
    "        del valid_dataset\n",
    "        return train_loader,valid_loader\n",
    "    ret = len(val_subs)+len(train_subs)\n",
    "    del train_ds\n",
    "    del valid_ds\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.util import set_random_seeds\n",
    "from multi_epoch.utils import *\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import wandb\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "PATH = '/scratch/sleepkfoldsame/'\n",
    "\n",
    "# Params\n",
    "SAVE_PATH = \"multi-epoch-kfold.pth\"\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 128\n",
    "lr = 5e-4\n",
    "n_epochs = 250\n",
    "NUM_WORKERS = 5\n",
    "N_DIM = 128\n",
    "EPOCH_LEN = 7\n",
    "\n",
    "\n",
    "class pretext_data(Dataset):\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        \n",
    "        self.file_path = filepath\n",
    "        self.idx = np.array(range(len(self.file_path)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        path = self.file_path[index]\n",
    "        data = np.load(path)\n",
    "        pos = data['pos']\n",
    "        neg = data['neg']\n",
    "        anc = copy.deepcopy(pos)\n",
    "        \n",
    "        for i in range(pos.shape[0]):\n",
    "            pos[i] = augment(pos[i])\n",
    "            anc[i] = augment(anc[i])\n",
    "            neg[i] = augment(neg[i])\n",
    "       \n",
    "        return anc, pos, neg\n",
    "    \n",
    "class train_data(Dataset):\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        \n",
    "        self.file_path = filepath\n",
    "        self.idx = np.array(range(len(self.file_path)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        path = self.file_path[index]\n",
    "        data = np.load(path)\n",
    "        \n",
    "        return data['x'], data['y']\n",
    "    \n",
    "    \n",
    "\n",
    "PRETEXT_FILE = os.listdir(os.path.join(PATH, \"pretext\"))\n",
    "PRETEXT_FILE.sort(key=natural_keys)\n",
    "PRETEXT_FILE = [os.path.join(PATH, \"pretext\", f) for f in PRETEXT_FILE]\n",
    "\n",
    "TEST_FILE = os.listdir(os.path.join(PATH, \"test\"))\n",
    "TEST_FILE.sort(key=natural_keys)\n",
    "TEST_FILE = [os.path.join(PATH, \"test\", f) for f in TEST_FILE]\n",
    "\n",
    "print(f'Number of pretext files: {len(PRETEXT_FILE)}')\n",
    "print(f'Number of test records: {len(TEST_FILE)}')\n",
    "\n",
    "pretext_loader = DataLoader(pretext_data(PRETEXT_FILE), batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "test_records = [np.load(f) for f in TEST_FILE]\n",
    "test_subjects = dict()\n",
    "\n",
    "for i, rec in enumerate(test_records):\n",
    "    if rec['_description'][0] not in test_subjects.keys():\n",
    "        test_subjects[rec['_description'][0]] = [rec]\n",
    "    else:\n",
    "        test_subjects[rec['_description'][0]].append(rec)\n",
    "\n",
    "test_subjects = list(test_subjects.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/home/zombie/multi-epoch/single_epoch_momentum')\n",
    "\n",
    "from augmentations import *\n",
    "from loss import loss_fn\n",
    "from model import sleep_model\n",
    "from train import *\n",
    "from utils import *\n",
    "\n",
    "from braindecode.util import set_random_seeds\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import wandb\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "PATH = '/mnt/scratch/sleepkfoldsame/'\n",
    "\n",
    "# Params\n",
    "SAVE_PATH = \"single-epoch-same.pth\"\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 128\n",
    "lr = 5e-4\n",
    "n_epochs = 200\n",
    "NUM_WORKERS = 5\n",
    "N_DIM = 256\n",
    "EPOCH_LEN = 7\n",
    "m = 0.9995\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "random_state = 1234\n",
    "sfreq = 100\n",
    "high_cut_hz = 30\n",
    "\n",
    "# Seeds\n",
    "rng = np.random.RandomState(random_state)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"GPU available: {torch.cuda.device_count()}\")\n",
    "\n",
    "set_random_seeds(seed=random_state, cuda=device == \"cuda\")\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "\n",
    "# Extract number of channels and time steps from dataset\n",
    "n_channels, input_size_samples = (2, 3000)\n",
    "model = sleep_model(n_channels, input_size_samples, n_dim = N_DIM)\n",
    "\n",
    "\n",
    "q_encoder = model.to(device)\n",
    "k_encoder = model.to(device)\n",
    "\n",
    "# for param_q, param_k in zip(q_encoder.parameters(), k_encoder.parameters()):\n",
    "#     param_k.data.copy_(param_q.data) \n",
    "#     param_k.requires_grad = False  # not update by gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling augmentation test\n",
    "import os\n",
    "import numpy as np\n",
    "PATH =  \"/mnt/scratch/sleepkfoldsame/test/731.npz\"\n",
    "data = np.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['windows'][0,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(x,degree_scale=2):\n",
    "    #eprint(x.shape)\n",
    "    ret = np.zeros_like(x)\n",
    "    degree = 0.05*(degree_scale+np.random.rand())\n",
    "    factor = 2*np.random.normal(size=x.shape[1])-1\n",
    "    factor = 1.5+(2*np.random.rand())+degree*factor\n",
    "    for i in range(x.shape[0]):\n",
    "        ret[i]=x[i]*factor\n",
    "    ret = torch.from_numpy(ret)\n",
    "    return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "original = data['windows'][0]\n",
    "scaled = scaling(original)\n",
    "plt.figure(figsize = (25, 10))\n",
    "plt.plot(original[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (25, 10))\n",
    "plt.plot(scaled[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_channel(ts, mode, degree):\n",
    "    \"\"\"\n",
    "    Add noise to ts\n",
    "    \n",
    "    mode: high, low, both\n",
    "    degree: degree of noise, compared with range of ts    \n",
    "    \n",
    "    Input:\n",
    "        ts: (n_length)\n",
    "    Output:\n",
    "        out_ts: (n_length)\n",
    "        \n",
    "    \"\"\"\n",
    "    len_ts = len(ts)\n",
    "    num_range = np.ptp(ts)+1e-4 # add a small number for flat signal\n",
    "    \n",
    "    ### high frequency noise\n",
    "    if mode == 'high':\n",
    "        noise = degree * num_range * (2*np.random.rand(len_ts)-1)\n",
    "        out_ts = ts + noise\n",
    "        \n",
    "    ### low frequency noise\n",
    "    elif mode == 'low':\n",
    "        noise = degree * num_range * (2*np.random.rand(len_ts//100)-1)\n",
    "        x_old = np.linspace(0, 1, num=len_ts//100, endpoint=True)\n",
    "        x_new = np.linspace(0, 1, num=len_ts, endpoint=True)\n",
    "        f = interp1d(x_old, noise, kind='linear')\n",
    "        noise = f(x_new)\n",
    "        out_ts = ts + noise\n",
    "        \n",
    "    ### both high frequency noise and low frequency noise\n",
    "    elif mode == 'both':\n",
    "        noise1 = degree * num_range * (2*np.random.rand(len_ts)-1)\n",
    "        noise2 = degree * num_range * (2*np.random.rand(len_ts//100)-1)\n",
    "        x_old = np.linspace(0, 1, num=len_ts//100, endpoint=True)\n",
    "        x_new = np.linspace(0, 1, num=len_ts, endpoint=True)\n",
    "        f = interp1d(x_old, noise2, kind='linear')\n",
    "        noise2 = f(x_new)\n",
    "        out_ts = ts + noise1 + noise2\n",
    "\n",
    "    else:\n",
    "        out_ts = ts\n",
    "\n",
    "    return out_ts\n",
    "\n",
    "def jitter(x, degree_scale=1):\n",
    "\n",
    "    #mode = np.random.choice(['high', 'low', 'both', 'no'])\n",
    "    mode = 'both' \n",
    "\n",
    "    ret = []\n",
    "    for chan in range(x.shape[0]):\n",
    "        ret.append(noise_channel(x[chan],mode,0.05*degree_scale))\n",
    "    ret = np.vstack(ret)\n",
    "    ret = torch.from_numpy(ret)\n",
    "    return ret \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "jittered = jitter(original)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (25, 10))\n",
    "plt.plot(original[0])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (25, 10))\n",
    "plt.plot(jittered[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1204c25c12f8e04c740b2ed7186598c25c3a2790f80406dfc9c3f371165071bb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
